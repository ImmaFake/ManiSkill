{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ca039b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import mani_skill.env\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "MAX_EPISODES = 700               # 最大训练代数\n",
    "MAX_EP_STEPS = 200               # episode最大持续帧数\n",
    "\n",
    "RENDER = True\n",
    "\n",
    "LOAD_MEM = False\n",
    "LOAD_MODEL = False\n",
    "LOAD_FOLDER_NAME = 'carbinet_1000epi'\n",
    "\n",
    "LOAD_PRETRAIN = True\n",
    "\n",
    "\n",
    "ENV_NAME = 'OpenCabinetDrawer-v0'         # 游戏名称\n",
    "SEED = 123                       # 随机数种子\n",
    "\n",
    "var = 5\n",
    "var_low_bound = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73be6930",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  DDPG  ####################################\n",
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound,):\n",
    "        self.a_dim = a_dim\n",
    "        self.s_dim = s_dim\n",
    "        self.a_bound = a_bound\n",
    "        self.pointer = 0                                                                         # exp buffer指针\n",
    "        self.lr_a = 0.001                                                                        # learning rate for actor\n",
    "        self.lr_c = 0.002                                                                        # learning rate for critic\n",
    "        self.gamma = 0.9                                                                         # reward discount\n",
    "        self.tau = 0.01                                                                          # 软更新比例\n",
    "        self.memory_capacity = 10000\n",
    "        self.batch_size = 32\n",
    "        self.memory = np.zeros((self.memory_capacity, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "\n",
    "        self.Reward_record = []\n",
    "        \n",
    "        class ANet(nn.Module):                               # 定义动作网络\n",
    "            def __init__(self, s_dim, a_dim, a_bound):\n",
    "                super(ANet,self).__init__()\n",
    "                self.a_bound = a_bound\n",
    "                self.fc1 = nn.Linear(s_dim,256)\n",
    "                self.fc1.weight.data.normal_(0,0.1)          # initialization\n",
    "                \n",
    "                self.fc2 = nn.Linear(256,512)\n",
    "                self.fc2.weight.data.normal_(0,0.1)\n",
    "                               \n",
    "                self.fc3 = nn.Linear(512,256)\n",
    "                self.fc3.weight.data.normal_(0,0.1)\n",
    "                \n",
    "                self.out = nn.Linear(256,a_dim)\n",
    "                self.out.weight.data.normal_(0,0.1)          # initialization\n",
    "            def forward(self,x):\n",
    "                x = self.fc1(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fc2(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fc3(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.out(x)\n",
    "                x = F.tanh(x)\n",
    "                actions_value = x * a_bound\n",
    "                return actions_value\n",
    "\n",
    "        class CNet(nn.Module):                               # 定义价值网络\n",
    "            def __init__(self,s_dim,a_dim):\n",
    "                super(CNet,self).__init__()\n",
    "                self.fcs1 = nn.Linear(s_dim,256)\n",
    "                self.fcs1.weight.data.normal_(0,0.1)          # initialization\n",
    "                \n",
    "                self.fcs2 = nn.Linear(256,512)\n",
    "                self.fcs2.weight.data.normal_(0,0.1) \n",
    "                \n",
    "                self.fcs3 = nn.Linear(512,256)\n",
    "                self.fcs3.weight.data.normal_(0,0.1) \n",
    "                \n",
    "                self.fca1 = nn.Linear(a_dim,256)\n",
    "                self.fca1.weight.data.normal_(0,0.1)          # initialization\n",
    "                \n",
    "                self.fca2 = nn.Linear(256,512)\n",
    "                self.fca2.weight.data.normal_(0,0.1)                \n",
    "                \n",
    "                self.fca3 = nn.Linear(512,256)\n",
    "                self.fca3.weight.data.normal_(0,0.1)\n",
    "                \n",
    "                self.out = nn.Linear(256,1)\n",
    "                self.out.weight.data.normal_(0, 0.1)         # initialization\n",
    "            def forward(self,s,a):\n",
    "                x = self.fcs1(s)                              # 输入状态\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fcs2(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fcs3(x)\n",
    "                x = F.relu(x) \n",
    "                \n",
    "                y = self.fca1(a)                              # 输入动作\n",
    "                y = F.relu(y)\n",
    "                \n",
    "                y = self.fca2(y)                              \n",
    "                y = F.relu(y)\n",
    "                \n",
    "                y = self.fca3(y)                              \n",
    "                y = F.relu(y)\n",
    "                \n",
    "                net = F.relu(x+y)\n",
    "                actions_value = self.out(net)                # 给出V(s,a)\n",
    "                return actions_value\n",
    "\n",
    "        self.Actor_eval = ANet(s_dim, a_dim, a_bound)        # 主网络\n",
    "        self.Actor_target = ANet(s_dim, a_dim, a_bound)      # 目标网络\n",
    "        self.Critic_eval = CNet(s_dim, a_dim)                # 主网络\n",
    "        self.Critic_target = CNet(s_dim, a_dim)              # 当前网络\n",
    "        self.ctrain = torch.optim.Adam(self.Critic_eval.parameters(),lr = self.lr_c) # critic的优化器\n",
    "        self.atrain = torch.optim.Adam(self.Actor_eval.parameters(),lr = self.lr_a)  # actor的优化器\n",
    "        self.loss_td = nn.MSELoss()                          # 损失函数采用均方误差\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = torch.unsqueeze(torch.FloatTensor(s), 0)\n",
    "        #print('choose action: ',self.Actor_eval(s))\n",
    "        #print('choose action 0 : ', self.Actor_eval(s)[0])\n",
    "        #print('forward: ',self.Actor_eval.forward(s))\n",
    "        #return self.Actor_eval(s)[0].detach()                # detach()不需要计算梯度\n",
    "        return self.Actor_eval.forward(s)[0].detach()\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        for x in self.Actor_target.state_dict().keys():\n",
    "            eval('self.Actor_target.' + x + '.data.mul_((1 - self.tau))')  \n",
    "            eval('self.Actor_target.' + x + '.data.add_(self.tau * self.Actor_eval.' + x + '.data)')\n",
    "        for x in self.Critic_target.state_dict().keys():\n",
    "            eval('self.Critic_target.' + x + '.data.mul_((1- self.tau))')\n",
    "            eval('self.Critic_target.' + x + '.data.add_(self.tau * self.Critic_eval.' + x + '.data)')\n",
    "\n",
    "        # soft target replacement\n",
    "\n",
    "        indices = np.random.choice(self.memory_capacity, size = self.batch_size)  # 随机采样的index\n",
    "        bt = self.memory[indices, :]                                              # 采样batch_size个sample\n",
    "        bs = torch.FloatTensor(bt[:, :self.s_dim])                                # state\n",
    "        ba = torch.FloatTensor(bt[:, self.s_dim: self.s_dim + self.a_dim])        # action\n",
    "        br = torch.FloatTensor(bt[:, -self.s_dim - 1: -self.s_dim])               # reward\n",
    "        bs_ = torch.FloatTensor(bt[:, -self.s_dim:])                              # next state\n",
    "\n",
    "        a = self.Actor_eval(bs)\n",
    "        q = self.Critic_eval(bs,a)  # loss=-q=-ce(s,ae(s))更新ae   ae(s)=a   ae(s_)=a_\n",
    "        # 如果 a是一个正确的行为的话，那么它的Q应该更贴近0\n",
    "        loss_a = -torch.mean(q) \n",
    "        # print(q)\n",
    "        # print(loss_a)\n",
    "        self.atrain.zero_grad()\n",
    "        loss_a.backward()\n",
    "        self.atrain.step()\n",
    "\n",
    "        a_ = self.Actor_target(bs_)      # 这个网络不及时更新参数, 用于预测 Critic 的 Q_target 中的 action\n",
    "        q_ = self.Critic_target(bs_,a_)  # 这个网络不及时更新参数, 用于给出 Actor 更新参数时的 Gradient ascent 强度\n",
    "        q_target = br + self.gamma * q_  # q_target = 负的\n",
    "        #print(q_target)\n",
    "        q_v = self.Critic_eval(bs,ba)\n",
    "        #print(q_v)\n",
    "        td_error = self.loss_td(q_target,q_v)\n",
    "        # td_error = R + self.gamma * ct（bs_,at(bs_)）-ce(s,ba) 更新ce ,但这个ae(s)是记忆中的ba，让ce得出的Q靠近Q_target,让评价更准确\n",
    "        #print(td_error)\n",
    "        self.ctrain.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.ctrain.step()\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        index = self.pointer % self.memory_capacity     # replace the old memory with new memory\n",
    "        self.memory[index, :] = transition\n",
    "        self.pointer += 1                               # 指示sample位置的指针+1\n",
    "    \n",
    "    def store_reward(self,ep_reward):\n",
    "        self.Reward_record.append(ep_reward)\n",
    "        \n",
    "    def save(self,folder_name):\n",
    "    \n",
    "        os.mkdir('./DPG model/' + folder_name)\n",
    "    \n",
    "        PATH1 = './DPG model/' + folder_name + '/Actor_eval.h5f'\n",
    "        PATH2 = './DPG model/' + folder_name + '/Actor_target.h5f'\n",
    "        PATH3 = './DPG model/' + folder_name + '/Critic_eval.h5f'\n",
    "        PATH4 = './DPG model/' + folder_name + '/Critic_target.h5f'\n",
    "    \n",
    "        torch.save(self.Actor_eval.state_dict(), PATH1)\n",
    "        torch.save(self.Actor_target.state_dict(), PATH2)\n",
    "        torch.save(self.Critic_eval.state_dict(), PATH3)\n",
    "        torch.save(self.Critic_target.state_dict(), PATH4)\n",
    "    \n",
    "        csv_name='./DPG model/' + folder_name + '/memory.csv'\n",
    "        xml_df = pd.DataFrame(self.memory)\n",
    "        xml_df.to_csv(csv_name, index = None, columns= None)\n",
    "        \n",
    "        csv_name='./DPG model/' + folder_name + '/Rewards.csv'\n",
    "        xml_df = pd.DataFrame(self.Reward_record)\n",
    "        xml_df.to_csv(csv_name, index = None, columns= None)\n",
    "        \n",
    "    def plot_file_rewards(self, folder_name):\n",
    "        data = pd.read_csv('./DPG model/' + folder_name + '/Rewards.csv')\n",
    "        y = np.array(data).squeeze()\n",
    "        x = range(y.shape[0])\n",
    "        plt.figure(figsize=(10, 10), dpi=70)\n",
    "        #plt.plot(x, y)\n",
    "        plt.scatter(x, y)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_rewards(self, scatter):\n",
    "        y = self.Reward_record\n",
    "        x = range(len(y))\n",
    "        plt.figure(figsize=(10, 10), dpi=70)\n",
    "        \n",
    "        if scatter:\n",
    "            plt.scatter(x, y)\n",
    "        else:\n",
    "            plt.plot(x, y)\n",
    "        plt.show()\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.Actor_eval.load_state_dict(torch.load('./DPG model/' + LOAD_FOLDER_NAME + '/Actor_eval.h5f'))\n",
    "        self.Actor_eval.eval()\n",
    "        self.Actor_target.load_state_dict(torch.load('./DPG model/' + LOAD_FOLDER_NAME + '/Actor_target.h5f'))\n",
    "        self.Actor_target.eval()\n",
    "        self.Critic_eval.load_state_dict(torch.load('./DPG model/' + LOAD_FOLDER_NAME + '/Critic_eval.h5f'))\n",
    "        self.Critic_eval.eval()\n",
    "        self.Critic_target.load_state_dict(torch.load('./DPG model/' + LOAD_FOLDER_NAME + '/Critic_target.h5f'))\n",
    "        self.Critic_target.eval()\n",
    "        print(\"Load network parameters of: \" + LOAD_FOLDER_NAME) \n",
    "        \n",
    "    def load_memory(self):\n",
    "        data = pd.read_csv('./DPG model/' + LOAD_FOLDER_NAME + '/memory.csv')\n",
    "        self.memory = np.array(data)\n",
    "        print(\"Load memory of: \" + LOAD_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8430a757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  Reward: -969 Explore: 0.01\n",
      "Episode: 1  Reward: -1218 Explore: 0.01\n",
      "Episode: 2  Reward: -1009 Explore: 0.01\n",
      "Episode: 3  Reward: -1021 Explore: 0.01\n",
      "Episode: 4  Reward: -1085 Explore: 0.01\n",
      "Episode: 5  Reward: -1007 Explore: 0.01\n",
      "Episode: 6  Reward: -1013 Explore: 0.01\n",
      "Episode: 7  Reward: -1007 Explore: 0.01\n",
      "Episode: 8  Reward: -1048 Explore: 0.01\n",
      "Episode: 9  Reward: -1050 Explore: 0.01\n",
      "Episode: 10  Reward: -990 Explore: 0.01\n",
      "Episode: 11  Reward: -1057 Explore: 0.01\n",
      "Episode: 12  Reward: -980 Explore: 0.01\n",
      "Episode: 13  Reward: -996 Explore: 0.01\n",
      "Episode: 14  Reward: -1026 Explore: 0.01\n",
      "Episode: 15  Reward: -1163 Explore: 0.01\n",
      "Episode: 16  Reward: -1187 Explore: 0.01\n",
      "Episode: 17  Reward: -1065 Explore: 0.01\n",
      "Episode: 18  Reward: -1062 Explore: 0.01\n",
      "Episode: 19  Reward: -1030 Explore: 0.01\n",
      "Episode: 20  Reward: -985 Explore: 0.01\n",
      "Episode: 21  Reward: -1180 Explore: 0.01\n",
      "Episode: 22  Reward: -985 Explore: 0.01\n",
      "Episode: 23  Reward: -1003 Explore: 0.01\n",
      "Episode: 24  Reward: -997 Explore: 0.01\n",
      "Episode: 25  Reward: -983 Explore: 0.01\n",
      "Episode: 26  Reward: -986 Explore: 0.01\n",
      "Episode: 27  Reward: -1086 Explore: 0.01\n",
      "Episode: 28  Reward: -992 Explore: 0.01\n",
      "Episode: 29  Reward: -1017 Explore: 0.01\n",
      "Episode: 30  Reward: -1165 Explore: 0.01\n",
      "Episode: 31  Reward: -991 Explore: 0.01\n",
      "Episode: 32  Reward: -1009 Explore: 0.01\n",
      "Episode: 33  Reward: -1006 Explore: 0.01\n",
      "Episode: 34  Reward: -1100 Explore: 0.01\n",
      "Episode: 35  Reward: -1094 Explore: 0.01\n",
      "Episode: 36  Reward: -1011 Explore: 0.01\n",
      "Episode: 37  Reward: -1027 Explore: 0.01\n",
      "Episode: 38  Reward: -1007 Explore: 0.01\n",
      "Episode: 39  Reward: -1019 Explore: 0.01\n",
      "Episode: 40  Reward: -1060 Explore: 0.01\n",
      "Episode: 41  Reward: -996 Explore: 0.01\n",
      "Episode: 42  Reward: -1096 Explore: 0.01\n",
      "Episode: 43  Reward: -1031 Explore: 0.01\n",
      "Episode: 44  Reward: -985 Explore: 0.01\n",
      "Episode: 45  Reward: -1072 Explore: 0.01\n",
      "Episode: 46  Reward: -1007 Explore: 0.01\n",
      "Episode: 47  Reward: -1006 Explore: 0.01\n",
      "Episode: 48  Reward: -1043 Explore: 0.01\n",
      "Episode: 49  Reward: -1012 Explore: 0.01\n",
      "Episode: 50  Reward: -1287 Explore: 0.01\n",
      "Episode: 51  Reward: -1490 Explore: 0.01\n",
      "Episode: 52  Reward: -1851 Explore: 0.01\n",
      "Episode: 53  Reward: -1512 Explore: 0.01\n",
      "Episode: 54  Reward: -1525 Explore: 0.01\n",
      "Episode: 55  Reward: -1350 Explore: 0.01\n",
      "Episode: 56  Reward: -1567 Explore: 0.00\n",
      "Episode: 57  Reward: -1485 Explore: 0.00\n",
      "Episode: 58  Reward: -1517 Explore: 0.00\n",
      "Episode: 59  Reward: -1563 Explore: 0.00\n",
      "Episode: 60  Reward: -1565 Explore: 0.00\n",
      "Episode: 61  Reward: -1780 Explore: 0.00\n",
      "Episode: 62  Reward: -1878 Explore: 0.00\n",
      "Episode: 63  Reward: -1501 Explore: 0.00\n",
      "Episode: 64  Reward: -1669 Explore: 0.00\n",
      "Episode: 65  Reward: -1572 Explore: 0.00\n",
      "Episode: 66  Reward: -1511 Explore: 0.00\n",
      "Episode: 67  Reward: -1377 Explore: 0.00\n",
      "Episode: 68  Reward: -1410 Explore: 0.00\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'set_scene'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_558506/3240433615.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EP_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FYD/Maniskill/mani_skill/env/open_cabinet_door_drawer.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcabinet\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArticulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticulations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cabinet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'articulation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FYD/Maniskill/mani_skill/env/base_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_cameras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_viewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_eval_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FYD/Maniskill/mani_skill/env/base_env.py\u001b[0m in \u001b[0;36m_setup_viewer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;31m# self._viewer.paused = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_scene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_camera_xyz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_camera_rpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mani_skill/lib/python3.8/site-packages/sapien/utils/viewer.py\u001b[0m in \u001b[0;36mset_scene\u001b[0;34m(self, scene)\u001b[0m\n\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscene\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_scene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps_camera_controller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFPSCameraController\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marc_camera_controller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArcRotateCameraController\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'set_scene'"
     ]
    }
   ],
   "source": [
    "###############################  training  ####################################\n",
    "env = gym.make(ENV_NAME)\n",
    "env = env.unwrapped\n",
    "env.seed(SEED)                                          # 设置Gym的随机数种子\n",
    "torch.manual_seed(SEED)                                 # 设置pytorch的随机数种子\n",
    "\n",
    "env.set_env_mode(obs_mode='state', reward_type='dense')\n",
    "\n",
    "s_dim = env.observation_space.shape[0]                  # 状态空间\n",
    "a_dim = env.action_space.shape[0]                       # 动作空间\n",
    "a_bound = 1                         # 动作取值区间,对称区间，故只取上界\n",
    "ddpg = DDPG(a_dim, s_dim, a_bound)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    ddpg.load_model()\n",
    "\n",
    "    \n",
    "if LOAD_MEM:\n",
    "    ddpg.load_memory()\n",
    "\n",
    "if LOAD_PRETRAIN:\n",
    "    ddpg.Actor_eval.load_state_dict(torch.load('./DPG model/pretrain/Actor_eval.h5f'))\n",
    "    ddpg.Actor_eval.eval()\n",
    "    ddpg.Actor_target.load_state_dict(torch.load('./DPG model/pretrain/Actor_eval.h5f'))\n",
    "    ddpg.Actor_target.eval() \n",
    "\n",
    "if LOAD_MODEL or LOAD_MEM:\n",
    "    var = 0.5\n",
    "\n",
    "if LOAD_PRETRAIN:\n",
    "    var = 0.01\n",
    "    \n",
    "t1 = time.time()                                        # 开始时间\n",
    "\n",
    "for i in range(MAX_EPISODES):\n",
    "    s = env.reset(level = 1)\n",
    "    ep_reward = 0\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "        if RENDER:\n",
    "            env.render('human')\n",
    "\n",
    "        # Add exploration noise\n",
    "        a = ddpg.choose_action(s)\n",
    "\n",
    "        a = np.clip(np.random.normal(a, var), -1, 1)    # add randomness to action selection for exploration\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "        \n",
    "        info_ = info.get('eval_info')\n",
    "        if info_.get('open_enough'):\n",
    "            print(\"!!!!!!!!!!!!!yeah!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            r += 10000\n",
    "        r += 7\n",
    "        \n",
    "        ddpg.store_transition(s, a, r, s_)         # 为什么要对reward归一化\n",
    "\n",
    "        if ddpg.pointer > ddpg.memory_capacity or LOAD_MEM:         # 经验池已满\n",
    "            if var > var_low_bound:\n",
    "                var *= .9995                            # 学习阶段逐渐降低动作随机性decay the action randomness\n",
    "            ddpg.learn()                                # 开始学习\n",
    "\n",
    "        s = s_\n",
    "        ep_reward += r\n",
    "        if j == MAX_EP_STEPS-1 or done:\n",
    "            print('Episode:', i, ' Reward: %i' % int(ep_reward), 'Explore: %.2f' % var, )\n",
    "            \n",
    "            ddpg.store_reward(ep_reward)\n",
    "            \n",
    "            if ep_reward > -300 or i == (MAX_EPISODES - 5):\n",
    "                RENDER = True\n",
    "                \n",
    "            break\n",
    "               \n",
    "print('Running time: ', time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e32791",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg.save('carbinet_1000epi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e826c974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ddpg.plot_rewards(scatter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9228e5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb78053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_info': {'cabinet_static': True, 'open_enough': False, 'success': False},\n",
       " 'dist_ee_to_handle': array([1.32828861, 1.30526796]),\n",
       " 'angle1': 1.1766494349134005,\n",
       " 'angle2': 1.5317321746066244,\n",
       " 'dist_ee_mid_to_handle': -1.3160184768477239,\n",
       " 'rew_ee_handle': -2.6335565692093796,\n",
       " 'rew_ee_mid_handle': -1.0,\n",
       " 'qpos_rew': 0,\n",
       " 'qvel_rew': 0,\n",
       " 'gripper_angle_err': 67.4170465869911,\n",
       " 'gripper_angle_rew': -1.1236174431165185,\n",
       " 'gripper_vel_norm': 0.0,\n",
       " 'gripper_ang_vel_norm': 0.0,\n",
       " 'qpos': 0.0,\n",
       " 'qvel': 0.0,\n",
       " 'target_qpos': 0.23135713040828706,\n",
       " 'reward_raw': -6.757174012325898,\n",
       " 'stage_reward': -7.0,\n",
       " 'TimeLimit.truncated': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3916b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41e5db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
