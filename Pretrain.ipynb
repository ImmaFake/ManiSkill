{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc07ad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import mani_skill.env\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "PATH = 'drawer_data.csv'\n",
    "ENV_NAME = 'OpenCabinetDrawer-v0'         # 游戏名称\n",
    "SEED = 123                       # 随机数种子\n",
    "\n",
    "class PreTrain(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound,):\n",
    "        \n",
    "        data = pd.read_csv(PATH)\n",
    "        self.memory = np.array(data)\n",
    "        \n",
    "        self.a_dim = a_dim\n",
    "        self.s_dim = s_dim\n",
    "        self.a_bound = a_bound\n",
    "        self.pointer = 0                                                                         # exp buffer指针\n",
    "        self.lr_a = 0.001                                                                        # learning rate for actor\n",
    "        self.gamma = 0.9                                                                         # reward discount\n",
    "        self.tau = 0.01                                                                          # 软更新比例\n",
    "        self.memory_capacity = 406357\n",
    "        self.batch_size = 32\n",
    "#         self.memory = np.zeros((self.memory_capacity, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "\n",
    "        self.Reward_record = []\n",
    "        \n",
    "        class ANet(nn.Module):                               # 定义动作网络\n",
    "            def __init__(self, s_dim, a_dim, a_bound):\n",
    "                super(ANet,self).__init__()\n",
    "                self.a_bound = a_bound\n",
    "                self.fc1 = nn.Linear(s_dim,512)\n",
    "                self.fc1.weight.data.normal_(0,0.1)          # initialization\n",
    "                \n",
    "                self.fc2 = nn.Linear(512,512)\n",
    "                self.fc2.weight.data.normal_(0,0.1)\n",
    "                               \n",
    "                self.fc3 = nn.Linear(512,512)\n",
    "                self.fc3.weight.data.normal_(0,0.1)\n",
    "                \n",
    "                self.out = nn.Linear(512,a_dim)\n",
    "                self.out.weight.data.normal_(0,0.1)          # initialization\n",
    "            def forward(self,x):\n",
    "                x = self.fc1(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fc2(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fc3(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.out(x)\n",
    "                x = F.tanh(x)\n",
    "                actions_value = x * a_bound\n",
    "                return actions_value\n",
    "\n",
    "\n",
    "\n",
    "        self.Actor_eval = ANet(s_dim, a_dim, a_bound)        # 主网络\n",
    "        self.atrain = torch.optim.Adam(self.Actor_eval.parameters(),lr = self.lr_a)  # actor的优化器\n",
    "        self.loss = nn.MSELoss()                          # 损失函数采用均方误差\n",
    "        \n",
    "    def train(self):\n",
    "            \n",
    "        for i in range(len(self.memory)):\n",
    "                \n",
    "            bt = self.memory[i, :]                                              # 采样batch_size个sample\n",
    "            bs = torch.FloatTensor(bt[:self.s_dim])                                # state\n",
    "            ba = torch.FloatTensor(bt[self.s_dim: self.s_dim + self.a_dim])        # action\n",
    "                \n",
    "            a = self.Actor_eval(bs)\n",
    "            loss_a = self.loss(ba, a)\n",
    "            \n",
    "            self.atrain.zero_grad()\n",
    "            loss_a.backward()\n",
    "            self.atrain.step()\n",
    "        \n",
    "    def save(self,folder_name):\n",
    "        os.mkdir('./DPG model/' + folder_name)\n",
    "        PATH1 = './DPG model/' + folder_name + '/Actor_eval.h5f'\n",
    "        torch.save(self.Actor_eval.state_dict(), PATH1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69976004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lue/anaconda3/envs/mani_skill/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: './DPG model/drawer_actor_pre'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1304032/3803141759.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mpre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drawer_actor_pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running time: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1304032/316970456.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, folder_name)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./DPG model/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mPATH1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./DPG model/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/Actor_eval.h5f'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActor_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './DPG model/drawer_actor_pre'"
     ]
    }
   ],
   "source": [
    "###############################  training  ####################################\n",
    "env = gym.make(ENV_NAME)\n",
    "env = env.unwrapped\n",
    "env.seed(SEED)                                          # 设置Gym的随机数种子\n",
    "torch.manual_seed(SEED)                                 # 设置pytorch的随机数种子\n",
    "\n",
    "env.set_env_mode(obs_mode='state', reward_type='dense')\n",
    "\n",
    "s_dim = env.observation_space.shape[0]                  # 状态空间\n",
    "a_dim = env.action_space.shape[0]                       # 动作空间\n",
    "a_bound = 1                         # 动作取值区间,对称区间，故只取上界\n",
    "\n",
    "pre = PreTrain(a_dim, s_dim, a_bound)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "pre.train()\n",
    "\n",
    "pre.save('drawer_actor_pre')\n",
    "print('Running time: ', time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945a553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import mani_skill.env\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "PATH = 'drawer_data.csv'\n",
    "ENV_NAME = 'OpenCabinetDrawer-v0'         # 游戏名称\n",
    "SEED = 123                       # 随机数种子\n",
    "\n",
    "class PreTrain(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound,):\n",
    "        \n",
    "        data = pd.read_csv(PATH)\n",
    "        self.memory = np.array(data)\n",
    "        \n",
    "        self.a_dim = a_dim\n",
    "        self.s_dim = s_dim\n",
    "        self.a_bound = a_bound\n",
    "        self.pointer = 0                                                                         # exp buffer指针\n",
    "        self.lr_c = 0.002                                                                        # learning rate for actor\n",
    "        self.gamma = 0.9                                                                         # reward discount\n",
    "        self.tau = 0.01                                                                          # 软更新比例\n",
    "        self.memory_capacity = 406357\n",
    "        self.batch_size = 32\n",
    "#         self.memory = np.zeros((self.memory_capacity, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "\n",
    "        self.Reward_record = []\n",
    "    \n",
    "        class ANet(nn.Module):                               # 定义动作网络\n",
    "            def __init__(self, s_dim, a_dim, a_bound):\n",
    "                super(ANet,self).__init__()\n",
    "                self.a_bound = a_bound\n",
    "                self.fc1 = nn.Linear(s_dim,256)\n",
    "                self.fc1.weight.data.normal_(0,0.1)          # initialization\n",
    "                \n",
    "                self.fc2 = nn.Linear(256,512)\n",
    "                self.fc2.weight.data.normal_(0,0.1)\n",
    "                               \n",
    "                self.fc3 = nn.Linear(512,256)\n",
    "                self.fc3.weight.data.normal_(0,0.1)\n",
    "                \n",
    "                self.out = nn.Linear(256,a_dim)\n",
    "                self.out.weight.data.normal_(0,0.1)          # initialization\n",
    "            def forward(self,x):\n",
    "                x = self.fc1(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fc2(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fc3(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.out(x)\n",
    "                x = F.tanh(x)\n",
    "                actions_value = x * a_bound\n",
    "                return actions_value\n",
    "    \n",
    "        class ANet(nn.Module):                               # 定义动作网络\n",
    "            def __init__(self, s_dim, a_dim, a_bound):\n",
    "                super(ANet,self).__init__()\n",
    "                self.a_bound = a_bound\n",
    "                self.fc1 = nn.Linear(s_dim,256)\n",
    "                self.fc1.weight.data.normal_(0,0.1)          # initialization\n",
    "                \n",
    "                self.fc2 = nn.Linear(256,512)\n",
    "                self.fc2.weight.data.normal_(0,0.1)\n",
    "                               \n",
    "                self.fc3 = nn.Linear(512,256)\n",
    "                self.fc3.weight.data.normal_(0,0.1)\n",
    "                \n",
    "                self.out = nn.Linear(256,a_dim)\n",
    "                self.out.weight.data.normal_(0,0.1)          # initialization\n",
    "            def forward(self,x):\n",
    "                x = self.fc1(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fc2(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fc3(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.out(x)\n",
    "                x = F.tanh(x)\n",
    "                actions_value = x * a_bound\n",
    "                return actions_value\n",
    "        \n",
    "        class CNet(nn.Module):                               # 定义价值网络\n",
    "            def __init__(self,s_dim,a_dim):\n",
    "                super(CNet,self).__init__()\n",
    "                self.fcs1 = nn.Linear(s_dim,256)\n",
    "                self.fcs1.weight.data.normal_(0,0.1)          # initialization\n",
    "                \n",
    "                self.fcs2 = nn.Linear(256,512)\n",
    "                self.fcs2.weight.data.normal_(0,0.1) \n",
    "                \n",
    "                self.fcs3 = nn.Linear(512,256)\n",
    "                self.fcs3.weight.data.normal_(0,0.1) \n",
    "                \n",
    "                self.fca1 = nn.Linear(a_dim,256)\n",
    "                self.fca1.weight.data.normal_(0,0.1)          # initialization\n",
    "                \n",
    "                self.fca2 = nn.Linear(256,512)\n",
    "                self.fca2.weight.data.normal_(0,0.1)                \n",
    "                \n",
    "                self.fca3 = nn.Linear(512,256)\n",
    "                self.fca3.weight.data.normal_(0,0.1)\n",
    "                \n",
    "                self.out = nn.Linear(256,1)\n",
    "                self.out.weight.data.normal_(0, 0.1)         # initialization\n",
    "            def forward(self,s,a):\n",
    "                x = self.fcs1(s)                              # 输入状态\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fcs2(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fcs3(x)\n",
    "                x = F.relu(x) \n",
    "                \n",
    "                y = self.fca1(a)                              # 输入动作\n",
    "                y = F.relu(y)\n",
    "                \n",
    "                y = self.fca2(y)                              \n",
    "                y = F.relu(y)\n",
    "                \n",
    "                y = self.fca3(y)                              \n",
    "                y = F.relu(y)\n",
    "                \n",
    "                net = F.relu(x+y)\n",
    "                actions_value = self.out(net)                # 给出V(s,a)\n",
    "                return actions_value\n",
    "\n",
    "\n",
    "        self.Actor_eval = ANet(s_dim, a_dim, a_bound)\n",
    "        self.Critic_eval = CNet(s_dim, a_dim)        # 主网络\n",
    "        self.ctrain = torch.optim.Adam(self.Critic_eval.parameters(),lr = self.lr_c)  # actor的优化器\n",
    "        self.loss = nn.MSELoss()                          # 损失函数采用均方误差\n",
    "        \n",
    "    def train(self):\n",
    "            \n",
    "        for i in range(len(self.memory)):\n",
    "                \n",
    "            bt = self.memory[i, :]                                              # 采样batch_size个sample\n",
    "            bs = torch.FloatTensor(bt[:self.s_dim])                                # state\n",
    "            ba = torch.FloatTensor(bt[self.s_dim: self.s_dim + self.a_dim])        # action\n",
    "            br = torch.FloatTensor(bt[-self.s_dim - 1: -self.s_dim])\n",
    "            bs_ = torch.FloatTensor(bt[-self.s_dim:])\n",
    "#             tmp = np.zeros(q, dtype=np.float32)\n",
    "            \n",
    "            a = self.Actor_eval(bs) \n",
    "            q = self.Critic_eval(bs, a)\n",
    "            \n",
    "            a_ = self.Actor_eval(bs_)\n",
    "            q_ = self.Critic_eval(bs_, a_)\n",
    "            \n",
    "            br += 10\n",
    "            br = br * 200\n",
    "            br += 200\n",
    "            \n",
    "            loss_td = q - (br + q_)\n",
    "            \n",
    "#             loss_td = self.loss(q, -q)\n",
    "            \n",
    "            self.ctrain.zero_grad()\n",
    "            loss_td.backward()\n",
    "            self.ctrain.step()\n",
    "        \n",
    "    def save(self,folder_name):\n",
    "        os.mkdir('./DPG model/' + folder_name)\n",
    "        PATH1 = './DPG model/' + folder_name + '/Critic_eval.h5f'\n",
    "        torch.save(self.Critic_eval.state_dict(), PATH1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f717af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lue/anaconda3/envs/mani_skill/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time:  2418.9997477531433\n"
     ]
    }
   ],
   "source": [
    "###############################  training  ####################################\n",
    "env = gym.make(ENV_NAME)\n",
    "env = env.unwrapped\n",
    "env.seed(SEED)                                          # 设置Gym的随机数种子\n",
    "torch.manual_seed(SEED)                                 # 设置pytorch的随机数种子\n",
    "\n",
    "env.set_env_mode(obs_mode='state', reward_type='dense')\n",
    "\n",
    "s_dim = env.observation_space.shape[0]                  # 状态空间\n",
    "a_dim = env.action_space.shape[0]                       # 动作空间\n",
    "a_bound = 1                         # 动作取值区间,对称区间，故只取上界\n",
    "\n",
    "pre = PreTrain(a_dim, s_dim, a_bound)\n",
    "\n",
    "pre.Actor_eval.load_state_dict(torch.load('./DPG model/actor_pre/Actor_eval.h5f'))\n",
    "pre.Actor_eval.eval()\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "pre.train()\n",
    "\n",
    "pre.save('critic_pre_r')\n",
    "print('Running time: ', time.time() - t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
