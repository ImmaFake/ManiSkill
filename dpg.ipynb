{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7fe84ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import mani_skill.env\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "MAX_EPISODES = 700               # 最大训练代数\n",
    "MAX_EP_STEPS = 200               # episode最大持续帧数\n",
    "\n",
    "RENDER = True\n",
    "\n",
    "LOAD_MEM = False\n",
    "LOAD_MODEL = False\n",
    "LOAD_FOLDER_NAME = 'drawer_1024'\n",
    "\n",
    "LOAD_PRETRAIN = True\n",
    "\n",
    "\n",
    "ENV_NAME = 'OpenCabinetDrawer-v0'         # 游戏名称\n",
    "SEED = 123                       # 随机数种子\n",
    "\n",
    "var = 0.01\n",
    "var_low_bound = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ba3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  DDPG  ####################################\n",
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound,):\n",
    "        self.a_dim = a_dim\n",
    "        self.s_dim = s_dim\n",
    "        self.a_bound = a_bound\n",
    "        self.pointer = 0                                                                         # exp buffer指针\n",
    "        self.lr_a = 0.001                                                                        # learning rate for actor\n",
    "        self.lr_c = 0.002                                                                        # learning rate for critic\n",
    "        self.gamma = 0.8                                                                         # reward discount\n",
    "        self.tau = 0.0001                                                                          # 软更新比例\n",
    "        self.memory_capacity = 10000\n",
    "        self.batch_size = 32\n",
    "        self.memory = np.zeros((self.memory_capacity, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "\n",
    "        self.Reward_record = []\n",
    "        \n",
    "        class ANet(nn.Module):                               # 定义动作网络\n",
    "            def __init__(self, s_dim, a_dim, a_bound):\n",
    "                super(ANet,self).__init__()\n",
    "                self.a_bound = a_bound\n",
    "                self.fc1 = nn.Linear(s_dim,256)\n",
    "                self.fc1.weight.data.normal_(0,0.1)          # initialization\n",
    "                \n",
    "                self.fc2 = nn.Linear(256,512)\n",
    "                self.fc2.weight.data.normal_(0,0.1)\n",
    "                               \n",
    "                self.fc3 = nn.Linear(512,256)\n",
    "                self.fc3.weight.data.normal_(0,0.1)\n",
    "                \n",
    "                self.out = nn.Linear(256,a_dim)\n",
    "                self.out.weight.data.normal_(0,0.1)          # initialization\n",
    "            def forward(self,x):\n",
    "                x = self.fc1(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fc2(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fc3(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.out(x)\n",
    "                x = F.tanh(x)\n",
    "                actions_value = x * a_bound\n",
    "                return actions_value\n",
    "\n",
    "        class CNet(nn.Module):                               # 定义价值网络\n",
    "            def __init__(self,s_dim,a_dim):\n",
    "                super(CNet,self).__init__()\n",
    "                self.fcs1 = nn.Linear(s_dim,256)\n",
    "                self.fcs1.weight.data.normal_(0,0.1)          # initialization\n",
    "                \n",
    "                self.fcs2 = nn.Linear(256,512)\n",
    "                self.fcs2.weight.data.normal_(0,0.1) \n",
    "                \n",
    "                self.fcs3 = nn.Linear(512,256)\n",
    "                self.fcs3.weight.data.normal_(0,0.1) \n",
    "                \n",
    "                self.fca1 = nn.Linear(a_dim,256)\n",
    "                self.fca1.weight.data.normal_(0,0.1)          # initialization\n",
    "                \n",
    "                self.fca2 = nn.Linear(256,512)\n",
    "                self.fca2.weight.data.normal_(0,0.1)                \n",
    "                \n",
    "                self.fca3 = nn.Linear(512,256)\n",
    "                self.fca3.weight.data.normal_(0,0.1)\n",
    "                \n",
    "                self.out = nn.Linear(256,1)\n",
    "                self.out.weight.data.normal_(0, 0.1)         # initialization\n",
    "            def forward(self,s,a):\n",
    "                x = self.fcs1(s)                              # 输入状态\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fcs2(x)\n",
    "                x = F.relu(x)\n",
    "                \n",
    "                x = self.fcs3(x)\n",
    "                x = F.relu(x) \n",
    "                \n",
    "                y = self.fca1(a)                              # 输入动作\n",
    "                y = F.relu(y)\n",
    "                \n",
    "                y = self.fca2(y)                              \n",
    "                y = F.relu(y)\n",
    "                \n",
    "                y = self.fca3(y)                              \n",
    "                y = F.relu(y)\n",
    "                \n",
    "                net = F.relu(x+y)\n",
    "                actions_value = self.out(net)                # 给出V(s,a)\n",
    "                return actions_value\n",
    "\n",
    "        self.Actor_eval = ANet(s_dim, a_dim, a_bound)        # 主网络\n",
    "        self.Actor_target = ANet(s_dim, a_dim, a_bound)      # 目标网络\n",
    "        self.Critic_eval = CNet(s_dim, a_dim)                # 主网络\n",
    "        self.Critic_target = CNet(s_dim, a_dim)              # 当前网络\n",
    "        self.ctrain = torch.optim.Adam(self.Critic_eval.parameters(),lr = self.lr_c) # critic的优化器\n",
    "        self.atrain = torch.optim.Adam(self.Actor_eval.parameters(),lr = self.lr_a)  # actor的优化器\n",
    "        self.loss_td = nn.MSELoss()                          # 损失函数采用均方误差\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = torch.unsqueeze(torch.FloatTensor(s), 0)\n",
    "        #print('choose action: ',self.Actor_eval(s))\n",
    "        #print('choose action 0 : ', self.Actor_eval(s)[0])\n",
    "        #print('forward: ',self.Actor_eval.forward(s))\n",
    "        #return self.Actor_eval(s)[0].detach()                # detach()不需要计算梯度\n",
    "        return self.Actor_eval.forward(s)[0].detach()\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        for x in self.Actor_target.state_dict().keys():\n",
    "            eval('self.Actor_target.' + x + '.data.mul_((1 - self.tau))')  \n",
    "            eval('self.Actor_target.' + x + '.data.add_(self.tau * self.Actor_eval.' + x + '.data)')\n",
    "        for x in self.Critic_target.state_dict().keys():\n",
    "            eval('self.Critic_target.' + x + '.data.mul_((1- self.tau))')\n",
    "            eval('self.Critic_target.' + x + '.data.add_(self.tau * self.Critic_eval.' + x + '.data)')\n",
    "\n",
    "        # soft target replacement\n",
    "\n",
    "        indices = np.random.choice(self.memory_capacity, size = self.batch_size)  # 随机采样的index\n",
    "        bt = self.memory[indices, :]                                              # 采样batch_size个sample\n",
    "        bs = torch.FloatTensor(bt[:, :self.s_dim])                                # state\n",
    "        ba = torch.FloatTensor(bt[:, self.s_dim: self.s_dim + self.a_dim])        # action\n",
    "        br = torch.FloatTensor(bt[:, -self.s_dim - 1: -self.s_dim])               # reward\n",
    "        bs_ = torch.FloatTensor(bt[:, -self.s_dim:])                              # next state\n",
    "\n",
    "        a = self.Actor_eval(bs)\n",
    "        q = self.Critic_eval(bs,a)  # loss=-q=-ce(s,ae(s))更新ae   ae(s)=a   ae(s_)=a_\n",
    "        # 如果 a是一个正确的行为的话，那么它的Q应该更贴近0\n",
    "        loss_a = -torch.mean(q) \n",
    "        # print(q)\n",
    "        # print(loss_a)\n",
    "        self.atrain.zero_grad()\n",
    "        loss_a.backward()\n",
    "        self.atrain.step()\n",
    "\n",
    "#         a_ = self.Actor_eval(bs_)\n",
    "#         q_ = self.Critic_eval(bs_,a_)\n",
    "        a_ = self.Actor_target(bs_)      # 这个网络不及时更新参数, 用于预测 Critic 的 Q_target 中的 action\n",
    "        q_ = self.Critic_target(bs_,a_)  # 这个网络不及时更新参数, 用于给出 Actor 更新参数时的 Gradient ascent 强度\n",
    "        q_target = br + self.gamma * q_  # q_target = 负的\n",
    "        #print(q_target)\n",
    "        q_v = self.Critic_eval(bs,ba)\n",
    "        #print(q_v)\n",
    "        td_error = self.loss_td(q_target,q_v)\n",
    "#         td_error = torch.mean(q_target - q_v)\n",
    "        # td_error = R + self.gamma * ct（bs_,at(bs_)）-ce(s,ba) 更新ce ,但这个ae(s)是记忆中的ba，让ce得出的Q靠近Q_target,让评价更准确\n",
    "        #print(td_error)\n",
    "        self.ctrain.zero_grad()\n",
    "        td_error.backward()\n",
    "        self.ctrain.step()\n",
    "        \n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        transition = np.hstack((s, a, [r], s_))\n",
    "        index = self.pointer % self.memory_capacity     # replace the old memory with new memory\n",
    "        self.memory[index, :] = transition\n",
    "        self.pointer += 1                               # 指示sample位置的指针+1\n",
    "    \n",
    "    def store_reward(self,ep_reward):\n",
    "        self.Reward_record.append(ep_reward)\n",
    "        \n",
    "    def save(self,folder_name):\n",
    "    \n",
    "        os.mkdir('./DPG model/' + folder_name)\n",
    "    \n",
    "        PATH1 = './DPG model/' + folder_name + '/Actor_eval.h5f'\n",
    "        PATH2 = './DPG model/' + folder_name + '/Actor_target.h5f'\n",
    "        PATH3 = './DPG model/' + folder_name + '/Critic_eval.h5f'\n",
    "        PATH4 = './DPG model/' + folder_name + '/Critic_target.h5f'\n",
    "    \n",
    "        torch.save(self.Actor_eval.state_dict(), PATH1)\n",
    "        torch.save(self.Actor_target.state_dict(), PATH2)\n",
    "        torch.save(self.Critic_eval.state_dict(), PATH3)\n",
    "        torch.save(self.Critic_target.state_dict(), PATH4)\n",
    "    \n",
    "        csv_name='./DPG model/' + folder_name + '/memory.csv'\n",
    "        xml_df = pd.DataFrame(self.memory)\n",
    "        xml_df.to_csv(csv_name, index = None, columns= None)\n",
    "        \n",
    "        csv_name='./DPG model/' + folder_name + '/Rewards.csv'\n",
    "        xml_df = pd.DataFrame(self.Reward_record)\n",
    "        xml_df.to_csv(csv_name, index = None, columns= None)\n",
    "        \n",
    "    def plot_file_rewards(self, folder_name):\n",
    "        data = pd.read_csv('./DPG model/' + folder_name + '/Rewards.csv')\n",
    "        y = np.array(data).squeeze()\n",
    "        x = range(y.shape[0])\n",
    "        plt.figure(figsize=(10, 10), dpi=70)\n",
    "        #plt.plot(x, y)\n",
    "        plt.scatter(x, y)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_rewards(self, scatter):\n",
    "        y = self.Reward_record\n",
    "        x = range(len(y))\n",
    "        plt.figure(figsize=(10, 10), dpi=70)\n",
    "        \n",
    "        if scatter:\n",
    "            plt.scatter(x, y)\n",
    "        else:\n",
    "            plt.plot(x, y)\n",
    "        plt.show()\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.Actor_eval.load_state_dict(torch.load('./DPG model/' + LOAD_FOLDER_NAME + '/Actor_eval.h5f'))\n",
    "        self.Actor_eval.eval()\n",
    "        self.Actor_target.load_state_dict(torch.load('./DPG model/' + LOAD_FOLDER_NAME + '/Actor_eval.h5f'))\n",
    "        self.Actor_target.eval()\n",
    "        self.Critic_eval.load_state_dict(torch.load('./DPG model/' + LOAD_FOLDER_NAME + '/Critic_eval.h5f'))\n",
    "        self.Critic_eval.eval()\n",
    "        self.Critic_target.load_state_dict(torch.load('./DPG model/' + LOAD_FOLDER_NAME + '/Critic_target.h5f'))\n",
    "        self.Critic_target.eval()\n",
    "        print(\"Load network parameters of: \" + LOAD_FOLDER_NAME) \n",
    "        \n",
    "    def load_memory(self):\n",
    "        data = pd.read_csv('./DPG model/' + LOAD_FOLDER_NAME + '/memory.csv')\n",
    "        self.memory = np.array(data)\n",
    "        print(\"Load memory of: \" + LOAD_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc497923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lue/anaconda3/envs/mani_skill/lib/python3.8/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  Reward: -41673 Explore: 0.01\n",
      "Episode: 1  Reward: -40775 Explore: 0.01\n",
      "Episode: 2  Reward: -41847 Explore: 0.01\n",
      "Episode: 3  Reward: -63286 Explore: 0.01\n",
      "Episode: 4  Reward: -38594 Explore: 0.01\n",
      "Episode: 5  Reward: -59703 Explore: 0.01\n",
      "Episode: 6  Reward: -58364 Explore: 0.01\n",
      "Episode: 7  Reward: -53030 Explore: 0.01\n",
      "Episode: 8  Reward: -60593 Explore: 0.01\n",
      "Episode: 9  Reward: -58689 Explore: 0.01\n",
      "Episode: 10  Reward: -44961 Explore: 0.01\n",
      "Episode: 11  Reward: -42819 Explore: 0.01\n",
      "Episode: 12  Reward: -47040 Explore: 0.01\n",
      "Episode: 13  Reward: -39823 Explore: 0.01\n",
      "Episode: 14  Reward: -46674 Explore: 0.01\n",
      "Episode: 15  Reward: -36143 Explore: 0.01\n",
      "Episode: 16  Reward: -39266 Explore: 0.01\n",
      "Episode: 17  Reward: -39315 Explore: 0.01\n",
      "Episode: 18  Reward: -49879 Explore: 0.01\n",
      "Episode: 19  Reward: -38750 Explore: 0.01\n",
      "Episode: 20  Reward: -62412 Explore: 0.01\n",
      "Episode: 21  Reward: -38290 Explore: 0.01\n",
      "Episode: 22  Reward: -45964 Explore: 0.01\n",
      "Episode: 23  Reward: -45677 Explore: 0.01\n",
      "Episode: 24  Reward: -38235 Explore: 0.01\n",
      "Episode: 25  Reward: -59032 Explore: 0.01\n",
      "Episode: 26  Reward: -39035 Explore: 0.01\n",
      "Episode: 27  Reward: -38998 Explore: 0.01\n",
      "Episode: 28  Reward: -47990 Explore: 0.01\n",
      "Episode: 29  Reward: -54212 Explore: 0.01\n",
      "Episode: 30  Reward: -39387 Explore: 0.01\n",
      "Episode: 31  Reward: -50946 Explore: 0.01\n",
      "Episode: 32  Reward: -37960 Explore: 0.01\n",
      "Episode: 33  Reward: -45146 Explore: 0.01\n",
      "Episode: 34  Reward: -40349 Explore: 0.01\n",
      "Episode: 35  Reward: -45107 Explore: 0.01\n",
      "Episode: 36  Reward: -39428 Explore: 0.01\n",
      "Episode: 37  Reward: -49310 Explore: 0.01\n",
      "Episode: 38  Reward: -40673 Explore: 0.01\n",
      "Episode: 39  Reward: -41511 Explore: 0.01\n",
      "Episode: 40  Reward: -44504 Explore: 0.01\n",
      "Episode: 41  Reward: -39964 Explore: 0.01\n",
      "Episode: 42  Reward: -44843 Explore: 0.01\n",
      "Episode: 43  Reward: -39722 Explore: 0.01\n",
      "Episode: 44  Reward: -57564 Explore: 0.01\n",
      "Episode: 45  Reward: -43297 Explore: 0.01\n",
      "Episode: 46  Reward: -43515 Explore: 0.01\n",
      "Episode: 47  Reward: -57045 Explore: 0.01\n",
      "Episode: 48  Reward: -47034 Explore: 0.01\n",
      "Episode: 49  Reward: -62467 Explore: 0.01\n",
      "Episode: 50  Reward: -45886 Explore: 0.01\n",
      "Episode: 51  Reward: -43808 Explore: 0.01\n",
      "Episode: 52  Reward: -48967 Explore: 0.01\n",
      "Episode: 53  Reward: -41320 Explore: 0.01\n",
      "Episode: 54  Reward: -46808 Explore: 0.01\n",
      "Episode: 55  Reward: -41855 Explore: 0.01\n",
      "Episode: 56  Reward: -46779 Explore: 0.01\n",
      "Episode: 57  Reward: -44583 Explore: 0.01\n",
      "Episode: 58  Reward: -40140 Explore: 0.01\n",
      "Episode: 59  Reward: -43415 Explore: 0.01\n",
      "Episode: 60  Reward: -46969 Explore: 0.01\n",
      "Episode: 61  Reward: -43568 Explore: 0.01\n",
      "Episode: 62  Reward: -59770 Explore: 0.01\n",
      "Episode: 63  Reward: -39734 Explore: 0.01\n",
      "Episode: 64  Reward: -44536 Explore: 0.01\n",
      "Episode: 65  Reward: -39385 Explore: 0.01\n",
      "Episode: 66  Reward: -41520 Explore: 0.01\n",
      "Episode: 67  Reward: -45139 Explore: 0.01\n",
      "Episode: 68  Reward: -45731 Explore: 0.01\n",
      "Episode: 69  Reward: -46658 Explore: 0.01\n",
      "Episode: 70  Reward: -60849 Explore: 0.01\n",
      "Episode: 71  Reward: -40607 Explore: 0.01\n",
      "Episode: 72  Reward: -39603 Explore: 0.01\n",
      "Episode: 73  Reward: -44646 Explore: 0.01\n",
      "Episode: 74  Reward: -39070 Explore: 0.01\n",
      "Episode: 75  Reward: -54761 Explore: 0.01\n",
      "Episode: 76  Reward: -39819 Explore: 0.01\n",
      "Episode: 77  Reward: -38850 Explore: 0.01\n",
      "Episode: 78  Reward: -38914 Explore: 0.01\n",
      "Episode: 79  Reward: -40320 Explore: 0.01\n",
      "Episode: 80  Reward: -43472 Explore: 0.01\n",
      "Episode: 81  Reward: -41603 Explore: 0.01\n",
      "Episode: 82  Reward: -39957 Explore: 0.01\n",
      "Episode: 83  Reward: -40142 Explore: 0.01\n",
      "Episode: 84  Reward: -45871 Explore: 0.01\n",
      "Episode: 85  Reward: -39247 Explore: 0.01\n",
      "Episode: 86  Reward: -40760 Explore: 0.01\n",
      "Episode: 87  Reward: -44152 Explore: 0.01\n",
      "Episode: 88  Reward: -39382 Explore: 0.01\n",
      "Episode: 89  Reward: -41437 Explore: 0.01\n",
      "Episode: 90  Reward: -45814 Explore: 0.01\n",
      "Episode: 91  Reward: -39349 Explore: 0.01\n",
      "Episode: 92  Reward: -62093 Explore: 0.01\n",
      "Episode: 93  Reward: -43466 Explore: 0.01\n",
      "Episode: 94  Reward: -47511 Explore: 0.01\n",
      "Episode: 95  Reward: -42203 Explore: 0.01\n",
      "Episode: 96  Reward: -43470 Explore: 0.01\n",
      "Episode: 97  Reward: -39356 Explore: 0.01\n",
      "Episode: 98  Reward: -38729 Explore: 0.01\n",
      "Episode: 99  Reward: -57896 Explore: 0.01\n",
      "Episode: 100  Reward: -38097 Explore: 0.01\n",
      "Episode: 101  Reward: -54296 Explore: 0.01\n",
      "Episode: 102  Reward: -40006 Explore: 0.01\n",
      "Episode: 103  Reward: -51879 Explore: 0.01\n",
      "Episode: 104  Reward: -43212 Explore: 0.01\n",
      "Episode: 105  Reward: -41906 Explore: 0.01\n",
      "Episode: 106  Reward: -44219 Explore: 0.01\n",
      "Episode: 107  Reward: -37251 Explore: 0.01\n",
      "Episode: 108  Reward: -63512 Explore: 0.01\n",
      "Episode: 109  Reward: -51359 Explore: 0.01\n",
      "Episode: 110  Reward: -57608 Explore: 0.01\n",
      "Episode: 111  Reward: -41017 Explore: 0.01\n",
      "Episode: 112  Reward: -41441 Explore: 0.01\n",
      "Episode: 113  Reward: -63994 Explore: 0.01\n",
      "Episode: 114  Reward: -62041 Explore: 0.01\n",
      "Episode: 115  Reward: -43343 Explore: 0.01\n",
      "Episode: 116  Reward: -42976 Explore: 0.01\n",
      "Episode: 117  Reward: -56161 Explore: 0.01\n",
      "Episode: 118  Reward: -46318 Explore: 0.01\n",
      "Episode: 119  Reward: -38053 Explore: 0.01\n",
      "Episode: 120  Reward: -40228 Explore: 0.01\n",
      "Episode: 121  Reward: -42186 Explore: 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2930956/4239498380.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mvar_low_bound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#                 var *= .9995                            # 学习阶段逐渐降低动作随机性decay the action randomness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                \u001b[0;31m# 开始学习\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2930956/3565992929.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mloss_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;31m#         a_ = self.Actor_eval(bs_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mani_skill/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mani_skill/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Adam does not support sparse gradients, please consider SparseAdam instead'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mani_skill/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"retains_grad\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n\u001b[1;32m    949\u001b[0m                           \u001b[0;34m\"attribute won't be populated during autograd.backward(). If you indeed want the gradient \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###############################  training  ####################################\n",
    "env = gym.make(ENV_NAME)\n",
    "env = env.unwrapped\n",
    "env.seed(SEED)                                          # 设置Gym的随机数种子\n",
    "torch.manual_seed(SEED)                                 # 设置pytorch的随机数种子\n",
    "\n",
    "env.set_env_mode(obs_mode='state', reward_type='dense')\n",
    "\n",
    "s_dim = env.observation_space.shape[0]                  # 状态空间\n",
    "a_dim = env.action_space.shape[0]                       # 动作空间\n",
    "a_bound = 1                         # 动作取值区间,对称区间，故只取上界\n",
    "ddpg = DDPG(a_dim, s_dim, a_bound)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    ddpg.load_model()\n",
    "\n",
    "    \n",
    "if LOAD_MEM:\n",
    "    ddpg.load_memory()\n",
    "\n",
    "if LOAD_PRETRAIN:\n",
    "    ddpg.Actor_eval.load_state_dict(torch.load('./DPG model/actor_pre/Actor_eval.h5f'))\n",
    "    ddpg.Actor_eval.eval()\n",
    "    ddpg.Actor_target.load_state_dict(torch.load('./DPG model/actor_pre/Actor_eval.h5f'))\n",
    "    ddpg.Actor_target.eval() \n",
    "    ddpg.Critic_eval.load_state_dict(torch.load('./DPG model/critic_pre/Critic_eval.h5f'))\n",
    "    ddpg.Critic_eval.eval()\n",
    "    ddpg.Critic_target.load_state_dict(torch.load('./DPG model/critic_pre/Critic_eval.h5f'))\n",
    "    ddpg.Critic_target.eval() \n",
    "\n",
    "if LOAD_MODEL or LOAD_MEM:\n",
    "    var = 0.5\n",
    "\n",
    "if LOAD_PRETRAIN:\n",
    "    var = 0.01\n",
    "    \n",
    "t1 = time.time()                                        # 开始时间\n",
    "\n",
    "for i in range(MAX_EPISODES):\n",
    "    s = env.reset(level = 1)\n",
    "    ep_reward = 0\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "        if RENDER:\n",
    "            env.render('human')\n",
    "\n",
    "        # Add exploration noise\n",
    "        a = ddpg.choose_action(s)\n",
    "\n",
    "        a = np.clip(np.random.normal(a, var), -1, 1)    # add randomness to action selection for exploration\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "        \n",
    "        info_ = info.get('eval_info')\n",
    "        if info_.get('open_enough'):\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!yeah!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "            r += 10000\n",
    "        r += 10\n",
    "        r *= 100\n",
    "        \n",
    "        ddpg.store_transition(s, a, r, s_)         # 为什么要对reward归一化\n",
    "\n",
    "        if ddpg.pointer > ddpg.memory_capacity or LOAD_MEM:         # 经验池已满\n",
    "            if var > var_low_bound:\n",
    "#                 var *= .9995                            # 学习阶段逐渐降低动作随机性decay the action randomness\n",
    "                ddpg.learn()                                # 开始学习\n",
    "\n",
    "        s = s_\n",
    "        ep_reward += r\n",
    "        if j == MAX_EP_STEPS-1 or done:\n",
    "            print('Episode:', i, ' Reward: %i' % int(ep_reward), 'Explore: %.2f' % var, )\n",
    "            \n",
    "            ddpg.store_reward(ep_reward)\n",
    "            \n",
    "            if ep_reward > -300 or i == (MAX_EPISODES - 5):\n",
    "                RENDER = True\n",
    "                \n",
    "            break\n",
    "               \n",
    "print('Running time: ', time.time() - t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
