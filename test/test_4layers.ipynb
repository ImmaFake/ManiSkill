{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7adfb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "import mani_skill.env\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from torch.distributions import Normal\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "MAX_EPISODES = 2000               # 最大训练代数\n",
    "MAX_EP_STEPS = 200               # episode最大持续帧数\n",
    "\n",
    "RENDER = True\n",
    "\n",
    "LOAD_MEM = True\n",
    "LOAD_MODEL = True\n",
    "LOAD_FOLDER_NAME = 'Check_Points/1900epi'\n",
    "\n",
    "ENV_NAME = 'OpenCabinetDrawer-v0'         # 游戏名称\n",
    "SEED = 123                       # 随机数种子\n",
    "\n",
    "var = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca757450",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################  DDPG  ####################################\n",
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound,hidden_dim):\n",
    "        self.a_dim = a_dim\n",
    "        self.s_dim = s_dim\n",
    "        self.a_bound = a_bound\n",
    "        self.hidden_dim = hidden_dim\n",
    "                                      \n",
    "\n",
    "        self.Reward_record = []\n",
    "        self.Success_record = np.zeros(MAX_EPISODES)\n",
    "        \n",
    "        class ANet(nn.Module):                               # 定义动作网络\n",
    "            def __init__(self, s_dim, a_dim, a_bound,hidden_size):\n",
    "                super(ANet,self).__init__()\n",
    "                self.a_bound = a_bound\n",
    "                \n",
    "                self.linear1 = nn.Linear(s_dim, hidden_size)\n",
    "                self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.linear4 = nn.Linear(hidden_size, a_dim)\n",
    "                \n",
    "                self.linear4.weight.data.uniform_(-0.03, 0.03)\n",
    "                self.linear4.bias.data.uniform_(-0.03, 0.03)\n",
    "\n",
    "            def forward(self,x):\n",
    "                x = F.relu(self.linear1(x))\n",
    "                x = F.relu(self.linear2(x))\n",
    "                x = F.relu(self.linear3(x))\n",
    "                x = F.tanh(self.linear4(x))\n",
    "                \n",
    "\n",
    "                actions_value = x * a_bound\n",
    "                return actions_value\n",
    "\n",
    "        class CNet(nn.Module):                               # 定义价值网络\n",
    "            def __init__(self,s_dim,a_dim,hidden_size):\n",
    "                super(CNet,self).__init__()\n",
    "                \n",
    "                self.linear1 = nn.Linear(s_dim + a_dim, hidden_size)\n",
    "                self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.linear3 = nn.Linear(hidden_size, hidden_size)\n",
    "                self.linear4 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "                self.linear4.weight.data.uniform_(-0.03,0.03)\n",
    "                self.linear4.bias.data.uniform_(-0.03,0.03)\n",
    "\n",
    "            def forward(self,s,a):\n",
    "        \n",
    "                x = torch.cat([s, a], 1)\n",
    "                x = F.relu(self.linear1(x))\n",
    "                x = F.relu(self.linear2(x))\n",
    "                x = F.relu(self.linear3(x))\n",
    "                x = self.linear4(x)\n",
    "                return x\n",
    "\n",
    "        self.Actor_eval = ANet(s_dim, a_dim, a_bound, hidden_dim)        # 主网络\n",
    "        self.Actor_target = ANet(s_dim, a_dim, a_bound, hidden_dim)      # 目标网络\n",
    "        self.Critic_eval = CNet(s_dim, a_dim,hidden_dim)                # 主网络\n",
    "        self.Critic_target = CNet(s_dim, a_dim,hidden_dim)              # 当前网络\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = torch.unsqueeze(torch.FloatTensor(s), 0)\n",
    "        return self.Actor_eval(s)[0].detach()                # detach()不需要计算梯度 #用forward等价（应该\n",
    "\n",
    "    def store_reward(self,ep_reward):\n",
    "        self.Reward_record.append(ep_reward)\n",
    "        \n",
    "    def Record_success(self,epi):\n",
    "        self.Success_record[epi] = 1\n",
    "    \n",
    "    def plot_rewards(self, scatter):\n",
    "        y = self.Reward_record\n",
    "        x = range(len(y))\n",
    "        plt.figure(figsize=(10, 10), dpi=70)\n",
    "        \n",
    "        if scatter:\n",
    "            plt.scatter(x, y)\n",
    "        else:\n",
    "            plt.plot(x, y)\n",
    "        plt.show()\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.Actor_eval.load_state_dict(torch.load('./DPG model/' + LOAD_FOLDER_NAME + '/Actor_eval.h5f'))\n",
    "        self.Actor_eval.eval()\n",
    "        self.Actor_target.load_state_dict(torch.load('./DPG model/' + LOAD_FOLDER_NAME + '/Actor_target.h5f'))\n",
    "        self.Actor_target.eval()\n",
    "        self.Critic_eval.load_state_dict(torch.load('./DPG model/' + LOAD_FOLDER_NAME + '/Critic_eval.h5f'))\n",
    "        self.Critic_eval.eval()\n",
    "        self.Critic_target.load_state_dict(torch.load('./DPG model/' + LOAD_FOLDER_NAME + '/Critic_target.h5f'))\n",
    "        self.Critic_target.eval()\n",
    "        print(\"Load network parameters of: \" + LOAD_FOLDER_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b0e6607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load network parameters of: Check_Points/1900epi\n",
      "SUCCESS\n",
      "Episode: 0  Reward: 619\n",
      "SUCCESS\n",
      "Episode: 1  Reward: 619\n",
      "SUCCESS\n",
      "Episode: 2  Reward: 619\n",
      "SUCCESS\n",
      "Episode: 3  Reward: 619\n",
      "SUCCESS\n",
      "Episode: 4  Reward: 619\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'set_scene'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3151608/3670166600.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FYD/Maniskill/mani_skill/env/open_cabinet_door_drawer.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcabinet\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArticulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticulations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cabinet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'articulation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FYD/Maniskill/mani_skill/env/base_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, level)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_cameras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_viewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_eval_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/FYD/Maniskill/mani_skill/env/base_env.py\u001b[0m in \u001b[0;36m_setup_viewer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;31m# self._viewer.paused = True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_scene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_camera_xyz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_camera_rpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mani_skill/lib/python3.8/site-packages/sapien/utils/viewer.py\u001b[0m in \u001b[0;36mset_scene\u001b[0;34m(self, scene)\u001b[0m\n\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscene\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_scene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps_camera_controller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFPSCameraController\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marc_camera_controller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArcRotateCameraController\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'set_scene'"
     ]
    }
   ],
   "source": [
    "###############################  training  ####################################\n",
    "env = gym.make(ENV_NAME)\n",
    "env = env.unwrapped\n",
    "env.seed(SEED)                                          # 设置Gym的随机数种子\n",
    "torch.manual_seed(SEED)                                 # 设置pytorch的随机数种子\n",
    "\n",
    "env.set_env_mode(obs_mode='state', reward_type='dense')\n",
    "\n",
    "s_dim = env.observation_space.shape[0]                  # 状态空间\n",
    "a_dim = env.action_space.shape[0]                       # 动作空间\n",
    "a_bound = 1                                             # 动作取值区间,对称区间，故只取上界\n",
    "\n",
    "hidden_dim = 512\n",
    "\n",
    "ddpg = DDPG(a_dim, s_dim, a_bound, hidden_dim)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    ddpg.load_model()\n",
    "\n",
    "    \n",
    "t1 = time.time()                                        # 开始时间\n",
    "\n",
    "for i in range(MAX_EPISODES):\n",
    "    \n",
    "    s = env.reset(level = 8)\n",
    "    \n",
    "    ep_reward = 0\n",
    "\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "        \n",
    "        if RENDER:\n",
    "            env.render('human')\n",
    "\n",
    "        a = ddpg.choose_action(s)\n",
    "        a = a.numpy()\n",
    "        s_, r, done, info = env.step(a)\n",
    "        \n",
    "        info_ = info.get('eval_info')\n",
    "        \n",
    "        if info_.get('success'):\n",
    "            print('SUCCESS')\n",
    "            ddpg.Record_success(i)\n",
    "            r += 100 #+200\n",
    "        \n",
    "        r += 10\n",
    "        r = r * 200\n",
    "        r+= 200\n",
    "        r += info.get('qpos')*4000      \n",
    "\n",
    "        s = s_\n",
    "        \n",
    "        ep_reward += r\n",
    "        \n",
    "        if j == MAX_EP_STEPS -1 or done:\n",
    "            print('Episode:', i, ' Reward: %i' % int(ep_reward/j))     \n",
    "            break\n",
    "               \n",
    "print('Running time: ', time.time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d46ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
